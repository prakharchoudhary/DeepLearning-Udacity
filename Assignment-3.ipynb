{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmnist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Model\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Inputs\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    reg = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # Variables\n",
    "    weights = tf.Variable(\n",
    "        tf.truncated_normal([image_size*image_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    # Training Computation\n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) + reg*tf.nn.l2_loss(weights)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "     # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(\n",
    "        tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 22.258760\n",
      "Minibatch accuracy: 7.8%\n",
      "Validation accuracy: 10.0%\n",
      "Minibatch loss at step 500: 3.004976\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 75.8%\n",
      "Minibatch loss at step 1000: 1.915320\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 78.6%\n",
      "Minibatch loss at step 1500: 1.207780\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 80.0%\n",
      "Minibatch loss at step 2000: 1.002489\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 80.9%\n",
      "Minibatch loss at step 2500: 0.832931\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 81.4%\n",
      "Minibatch loss at step 3000: 0.790169\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 81.3%\n",
      "Test accuracy: 88.6%\n"
     ]
    }
   ],
   "source": [
    "num_steps= 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, reg: 1e-3}\n",
    "        _, l, predictions = session.run(\n",
    "            [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "                valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Let us plot the accuracy for various values of regularization\"\"\"\n",
    "import numpy as np\n",
    "import sys\n",
    "num_steps = 3001\n",
    "reg_vals = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "acc_val = []\n",
    "\n",
    "for reg_i in reg_vals:\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        for step in range(num_steps):\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, reg: reg_i}\n",
    "            _, l, predictions = session.run(\n",
    "                [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        acc_val.append(accuracy(test_prediction.eval(), test_labels))\n",
    "print(len(acc_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEMCAYAAAAoB2Y1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xd8HNW1wPHfkVbVqm6yLbn3XhDGYGwEmBYIhBJCMRAI\nGAMJeSQhkATCCwmER0IeyQvNmBKaaabZdALCNNvIxpLc5G5ZxZZkWb1L9/0xI7LIKivtStvO9/PZ\nj3Zn7tw5szs6O3vnzh0xxqCUUio4hHg7AKWUUn1Hk75SSgURTfpKKRVENOkrpVQQ0aSvlFJBRJO+\nUkoFEU36yueJSKSIGBFJ8XYs3SUia0VksRvL7xaR4z0cU4SIVInIME/W61T//4rIUvv5mSKyywN1\n9jhmEfmDiPzThXIPicjVPYvQf2jS9wB7Z2x9tIhIrdPry92o162EofyfMWasMeYrd+poux8ZY+qN\nMTHGmAL3IzxqXcnARcCTnqzX1Zjb+5IxxtxljPmpC6v5C3CXiIS6E6uv06TvAfbOGGOMiQFyge87\nTXve2/H1FhFxeDsGd/nqNvhqXC64BnjDGNPg7UC6yxizDzgAnOXlUHqVJv0+ICKhInKniOwRkRIR\neV5EEux5/UTkRREpFZEyEVknIoki8gBwLLDc/sXwQDv1OkRkpYgcspf9REQmOs3vJyL/EJEDIlIu\nIp+2JhMRSbOPAMtFJFdELrOnf+eoUESWishH9vPWZpYbRGQ3sNme/oiI5IlIhYisF5F5bWK8y972\nChH5WkSGiMgTInJPm+35QERu6OSt/IGI7BORYhG5RyzRdr3jnepJEZGa1ve4zTqWisjH9k/5I8Dt\n9vTrRSTH/hzeto9YW5c5W0R22u/xg87vkYjcJyLLncpOEpGm9oK356Xb6ygWkX+JSKzT/IMi8isR\n2QJUOE070d6HnH9RVtufxRARGSQi79p1lorImyIy1F7+qP1I2jSXiUh/EXnBXn6viPxaRMTp/fq3\nvR+VidXctKiTz+gs4NOOZorIdBH5zK4rS0TOcpo32N6OCvs9vq+dfa815vNEZLuIVNr7980iMgB4\nHRjj9D4NaOczanfft6UDZ3eyff7PGKMPDz6AfcCiNtNuAz4DhgGRwNPAU/a8nwOvAlGAA+sftJ89\nby2wuJN1OYArgRi73keAtU7znwA+AIYAocAC++84oAq40K5jEDCzvXUCS4GP7OeRgAHeBhKAKHv6\nlUAiEAb8DutoKcyedyfwjb3OEGC2vexCYC8gdrlhQA3Qv53tbF3v+/ayo4E9rXFiNSX8oc37/UoH\n79lSoAm4zn4vooAfAduACfY2/An4xC4/1H6vzrHn/RpodFr3fcByp/onAU1Or9c6lZ0EnAKE25/J\nWuA+p7IHga/t9yLKadqJ7WzH34CP7G1IAs6ztyUeeBN4sb0Y2ryfKfbrl4FX7P1onP25XO70fjXa\nn3EocAuwr5N9shKY7vT6TGCX03pzgV/a7+UZ9ns72p7/BvCMvR0zgEKO3vdaYz4MzLWfDwBmt12f\nUwzffkZ0su/b8y8DvvR2HunNh9cDCLQH7Sf9vcB8p9ejsRKcADdiHRlNa6euTpN+O+WHAC32P0iY\n/c86sZ1yfwBWdFCHK0n/hE5iEHvbJtqv9wNndFBuD7DAfv0r4LUO6mxdb5rTtF8Ab9vPT3L+Rwey\ngXM7qGspsKPNtE9ak5z9uvW9SwKWYH8B2PNCgCJ6kPTbieUS4Cun1weBy9qUOSrpYyXgXbTzBWnP\nnwcUdvKZfptAgQigGRjjNP/nwHtO79dmp3n97WUT2llvqD1vlNM056R/mr0/iNP817F+bUXa++5I\np3l/bWffa036RcDVQGybGLpK+h3u+/b87wNbXf2f88eHNu/0Mvtn8nDgHfsnbRnWkW8I1hHKE1hJ\n/1W7ieRecfFEkt108kBr0wmwHSuZDsA6QnUAu9tZdHgH0111oE0cv7GbRsqBI1j/oAPtbU9ub13G\n+g97BmhtSloMPNuN9e7HOiIGWAOEisjxIjILa9vfdTV+YCTwqNPnU4z1ayDFXse35Y0xLUB+F3G2\nS0SGicgrIpJvf17LgYFdxNa2juOAB4DzjDGl9rRYEXnSbqqowPp117bejgzB2hdznabtx/rcWh10\nel5j/41pW5ExphnrSD+27TzbMCDX/uzbrmsI1r6b5zSvs/fiPKyj9Vy7ue7YTso662rfjwXKXKzL\nL2nS72X2Dp4PnGKMSXB6RBpjSozVK+H3xphJWE0eP8Q6AgTryKYzV2MdPZ2M9bN+kj1dsH4aNwFj\n21nuQAfTAaqBaKfXQ9rbrNYnInIa8DPgfKyml/5ALdbRXOu2d7SuZ4CLROQYrH/Gtzso12q40/MR\nQAEc9QVyBVbTRmMn9bR9Xw8AP27z+UQZYzZgvY/fdhUVkRC+mxBdeb9a/cUuP80YEwdci/VZdRbb\nt8TqrrgSuNYYs8Vp1u12jMfa9Z7ept7O9qODWEfYI5ymjaCHX2xAFlYzWXsK2qzHeV0HseJ0fm+H\n0wFjzFfGmHOwfo19ALzQOquL+Drb9wEmA5ld1OHXNOn3jUeB+0RkOHx7wur79vNFIjLFTiYVWIm6\nxV7uEDCmk3pjgTqs9s1+WG3RANhJ7xng7yKSZJ8IPNH+FfEscI6InG//WhgkIjPsRTdhJeJIEZkE\n/LiLbYvFagopxmqrvhvrSL/VcuBeERkjltlin2A1xuwBtgJPAS+Zrnt83CYi8SIyCvgp8JLTvGeA\ni4FL7efd8Shwh9gnwcU6kX6hPe8t4DgR+Z5YJ8F/gXX+otUm4GQRSRaRRKzzCR2JxWpPrhCREXZd\nLhGRcOA14DFjzJvt1FsDlInIQOCONvM73I+MMfVYTSz3inXifyxW885zrsbWxjtYzW3t+QwIEZH/\nsve707C+oF42xtQBq4A/2PveNKz29aPYcV4iInFY+14l3/2fGSwiR/0SsXW272PH3tmvRL+nSb9v\n3I910u1jEakEvgTm2POSsU68VWL1hnmH/ySz/wWuFJEjInJ/O/U+gZVsD2K1Y3/eZv7NWD9lv8H6\nYvgj1hH4Lqyfx78FSoEMYKpTrA673mV0/c+/Cqt5ZTdWG32JvWyr+7CO4D/G+lJ7FKsdudW/gOl0\n3bSDXU+mHe8rzrEZY3YDOUClMWa9C3V9yxizAvgn8JrdPLIJ6xcUxphCrC+Sf9jbloL1Xtc7xbQa\n68trLdbJyI78HjgRKMdKtCu7EeYY4DisLz7nXjyDsdq+B2J9xp9j7UPOutqPrrf/7sf6nJYDPe1q\n/DRWL6vwtjPsxH4OVj/+w1gno39kf/m3xjEMa/9ZDqzgP+9zW9fY8ZZjneO40p6eifVFvd9uruvf\nJoYO930RGYnV1NfVL06/1tpzQimvEJHTgYeNMeM8UNcLWCfh/tRl4Z6vw4H1Jft94+ZFU4FKRP6G\ndbL8UTfr+TsQaYy5vsvCHiAiDwEbjDEevbDM12jSV17j1GSxxhjT3hFod+oaB2wEJhtjetoe3VHd\nZ2H9OqvH6pJ6FTDOheYo1Q12k47B+tV0PNYR96XGmPe8GliA0eYd5RV2L5sjWO3RD7lZ1/1YTVh3\nezrh21qvKSgCTgXO14TfK+KxmgursZru/qQJ3/P0SF8ppYKIHukrpVQQ0aSvlFJBxOdG8hs4cKAZ\nNWpUj5evrq6mX79+ngtIqW7Q/U95y4YNG0qMMYO6KudzSX/UqFFkZGT0ePn09HTS0tI8F5BS3aD7\nn/IWEdnvSjlt3lFKqSCiSV8ppYKIJn2llAoimvSVUiqIaNJXSqkgoklfKaWCiCZ9pTxkX0k11Y06\nrInybZr0lfKApuYWLnzkS/75TR06npXyZS4lfRG5RUS2iMhmEVlh39nmFBHZaE/7lz3OeHvLXiUi\nO+3HVZ4NXynfkJlXxuHqBraVtvDZzhJvh6NUh7pM+iKSjHUHplRjzDSsO95fhnXHo0vsafuxxhhv\nu2x/4C6sO/7MBe6ybymnVEBJzykmRKB/pPA/722npUWP9pVvcrV5xwFE2Ufz0VjjXTcYY3bY8z/E\nujN9W2cAHxpjSo0xR+xyZ7oZs1I+Jz2nmNkjErloQjhbCip4O7vQ2yEp1a4ux94xxuSLyF+BXKAW\n687zLwP3i0iqMSYD656X7d25Phnr7vOt8vju3e4BEJElwBKApKQk0tPTu7kZ/1FVVeXW8kp1V3m9\nITu/hgvGhzEttoHhsaH86c1NRB3OwREi3g5Pqe/oMunbzTHnAaOBMqwbUl8OXAL8r4hEYH0RNPc0\nCGPMMqybcJOammrcGbBKB7xSfW3lhjwgk6vPOI7Du77h7guncPXTX1MYPYYr5o30dnhKfYcrzTuL\ngL3GmGJjTCPWPU1PMMZ8ZYxZYIyZC6wBdrSzbD7f/QWQYk9TKmCk7yhmYEw4U4fFAZA2cRBzR/Xn\nH//eSU1Dk5ejU+q7XEn6ucA8EYkWEcG6R+g2ERkMYB/p3wY82s6y7wOni0ii/YvhdHuaUgGhucXw\n2c5iFk4YRIjdlCMi3HbWJIor63ny871ejlCp7+oy6Rtj1gGvAhuBbHuZZcCtIrINyAJWGWM+BhCR\nVBFZbi9bCvwR+Np+3G1PUyogbDpQRllNI2kTB39n+jEjEzltShKPfbqHI9V6D3XlO1zqvWOMucsY\nM8kYM80Yc4Uxpt4Yc6sxZrIxZqIx5kGnshnGmGudXj9pjBlnP57qjY1Qyls+zSkiRGDh+IFHzbv1\njIlUNzTxcPouL0SmVPv0ilyl3JC+o5hZwxNIiA4/at6EpFgumJPCv77aT35ZrReiU+pomvSV6qGS\nqnqy8sqPatpxdstpEwB48MP2+jko1fc06SvVQ5/tLAbgpAkd34s6OSGKK+eNZOXGPHYequyr0JTq\nkCZ9pXooPaeYAf3CmZ4c32m5m04eR79wB/e/n9NHkSnVMU36SvVAc4thzY7vdtXsSGK/cK4/aQwf\nbj3Ehv1H+ihCpdqnSV+pHsjKK+NITSNpEztu2nF2zYmjGRgTwf+8t12HXlZepUlfqR5IzylGBBaM\ndy3pR4c7+Pmp41i/t5T0nOJejk6pjmnSV6oH0ncUMzMlgf79ju6q2ZFL5o5g5IBoHXpZeZUmfaW6\n6XBVPVl5ZS437bQKCw3hl6dPZPvBSt7M1CGolHdo0leqmz7bWYIxdNo/vyPnTB/K1GFxPPDBDuqb\nejwwrVI9pklfqW5Kzymif79wZnTRVbM9ISHCr8+cRN6RWlasy+2F6JTqnCZ9pbqhpcWwZmcJC8cP\n7LKrZkcWjh/I8WMG8H8f76KqXodeVn1Lk75S3ZCVX05pdUOPmnZatQ69fLi6geWf7fFgdEp1TZO+\nUt2QnlOECCzsZOgFV8wansBZ04bw+Jo9lFTVeyg6pbqmSV+pbkjPKWZGN7tqduSXp0+ktrGZf36s\nQy+rvqNJXykXlVY3kJlXRpqbR/mtxg2O4eLU4Ty/bj8HSms8UqdSXdGkr5SLPttZbHfV9EzSB/iv\nRRMIEeF/dehl1Uc06SvlovScYhKjw5iRkuCxOofER/Lj+aN4fVM+2worPFavUh3RpK+UC1qcRtUM\n7WFXzY7ceNI4YiMc/EWHXlZ9QJO+Ui7YXFDO4eoGjzbttIqPDmNp2lg+3l7E8+v2s35vKdsKK8g7\nUkN5baOO06M8yuHtAJTyB62jai50cVTN7rr6hNG8sC6X372+ud35MREOYiNbH2Ft/jqIs5+nTRjM\niAHRvRKjCgya9JVyQXpOETOS4xkQE9Er9UeFh/Luzxewu7iayrpGKuuavv1bYT+vqmuyptc3Ulrd\nwP7DNVTWNVJR10RDUwsAidE7eP7aeUwZFtcrcSr/p0lfqS6U1TSw6UAZPz1lfK+uJzYyjFnDe3aS\nuL6pmX0lNVz91HouX76W5649jqnDuj82kAp82qavVBfW7CyhxcNdNT0twhHKxCGxvLjkeKLCQrl8\n+To255d7OyzlgzTpK9WF9JwiEqPDmOnBrpq9ZcSAaF5ccjz9wh2a+FW7NOkr1YnWrpoLxnu+q2Zv\nsRL/PGIiNPGro2nSV6oTWwoqKKnqna6avWl4//8k/sseX0t2niZ+ZdGkr1Qn0nOKAPdH1fSG4f2j\neen6ecRFhXH58rVk5ZV5OyTlAzTpK9WJ9B3FTE+OZ2AvddXsbSmJ1hF/fHQYly9fR+YBTfzBTpO+\nUh0oq2ngm9wjfte005aV+I8nMTqcxcvXsUkTf1DTpK9UBz7zg66arkpOiOLFJfNI7BfOFcvX8U3u\nEW+HpLxEk75SHUjPKSY+KoxZwxO9HYpHDEuI4qXr59E/JpwrnljPRk38QUmTvlLtaGkxfLqjmAXj\nB/pNV01XDI23jvgHxoRz5RPr2bBfE3+w0aSvVDu2FlZQUlXv1g3QfZWV+I9nUGwEVz6xjg37S70d\nkupDmvSVakdrV82T/LCrpiuGxEey4rp5DI6L5Mon1pOxTxN/sNCkr1Q70nOKmZYcx6BY/+yq6Yoh\n8ZG8uGQeSXGRXPnker7WxB8UNOkr1UZ5TSMbc4+QNiHwmnbaSoqzEv+Q+EiuenI96/Yc9nZIqpdp\n0leqjc93BU5XTVcMjovkxevmMTQ+kqueWs8ndtOWCkwuJX0RuUVEtojIZhFZISKRInKqiGwUkU0i\n8rmIjGtnuVEiUmuX2SQij3p+E5TyrPScIuIiHT0e294fDY6L5OXrj2fsoBiu+1cGq7MKvB2S6iVd\nJn0RSQZuBlKNMdOAUOAS4BHgcmPMLOAF4I4OqthtjJllP5Z6KG6leoUxdlfNCYNwhAbXD+EBMRGs\nWDKP2SMS+NmKb1ixPtfbIale4Ope7QCiRMQBRAMFgAFa78kWb09Tyq9tLaygqLKetADttdOVuMgw\nnrnmOE6aMIjfvJbNsjW7vR2S8rAub5dojMkXkb8CuUAt8IEx5gMRuRZ4R0RqgQpgXgdVjBaRb+wy\ndxhjPmtbQESWAEsAkpKSSE9P79HGAFRVVbm1vApuq3c3ABB2eCfp6d1PeIGy/y0eaagpD+Xed7aT\ntX03F44PQyRwLlILZmKM6byASCKwEvgRUAa8ArwKXAD8jzFmnYjcCkw0xlzbZtkIIMYYc1hEjgHe\nAKYaYyo6Wl9qaqrJyMjo8Qalp6eTlpbW4+VVcLv40a+obmji7ZsX9Gj5QNr/mlsMd7yRzYr1B7hi\n3kj+cO5UQgLo6uRAIyIbjDGpXZVz5cboi4C9xphiu+LXgPnATGPMOrvMS8B7bRc0xtQD9fbzDSKy\nG5gA9DyrK9VLymsb2ZB7hKUnjfF2KD4hNES49/zpxEWG8diaPVTVN3H/RTMIC7JzHYHGlU8vF5gn\nItFi/b47FdgKxIvIBLvMacC2tguKyCARCbWfjwHGA3s8ErlSHvbFrhKaW0xADr3QUyLC7WdN4tYz\nJvL6N/nc8NwG6hqbvR2WcoMrbfrrRORVYCPQBHwDLAPygJUi0gIcAa4BEJFzsXr6/B5YCNwtIo1A\nC7DUGKOX/Smf1NpVc3YQddV0hYhw08njiIt0cOebW7j6qa95/KpUYiJcaShQvsalT80YcxdwV5vJ\nr9uPtmXfAt6yn6/EOh+glE8rqqzjw62HWDA++LpquuqK40cRE+ngV69kcfnja3n66rkk9gv3dliq\nm3TvVkGvsbmFn77wDbWNzfzs1KOuMVROzp+dwqOLj2HbwUoufuwrDlXUeTsk1U2a9FXQu/+97azf\nW8p9F8xg0pC4rhcIcqdNSeLpq4+loKyWix79ktzDNd4OSXWDJn0V1N7JLuTxz/Zy5fEj+cHsZG+H\n4zdOGDuQ56+bR2VdExc9+iU5Byu9HZJykSZ9FbR2FVVx6yuZzB6RwB1nT/F2OH5n1vAEXr7+eAAu\nfuwrveG6n9Ckr4JSdX0TS5/bQGRYKA9fPodwh/4r9MSEpFheXXoC8VFhXP74Wl5Yl0vu4Rq6uuhT\neY/2uVJBxxjDbSuz2FNcxXM/OY6h8VHeDsmvjRgQzStLj+eqJ9fz29ezARgcG8Gxo/qTOiqRY0f1\nZ9KQWO0V5SM06Suf8cK6XMYM6se8MQN6dT1PfbGP1VmF/PrMiZwwbmCvritYJMVF8s7NC9hRVEnG\nviNk7Cvl631HeDu7EIB+4aHMGZlI6sj+HDsqkVkjEogO1/TjDfquK5+QnVfOb1/PJkTgjrOncPX8\nUb0ywFfGvlLufWcbp01J4oaTxnq8/mAWEiJMGhLHpCFxLJ43EoCCsloy9v/nS+DBf+/AGGuIh2nD\n4jjG/hI4ZlQig2MjvbwFwUGTvvIJD6fvIjbSwXGjB3D36q1sP1jBH38wjQhHqMfWUVRZx43PbyQl\nMYoHLp6po0b2gWEJUZybEMW5M4cBUFHXyMb9R6xfA/tLeX7dfp78Yi8AowZE88PU4dxw0lgd2K0X\nadJXXrerqIr3thzkprRx/OK0CTz40Q7+8fEudhdX88jiOR45AmxqbuFnL3xDRV0j/7pmLnGRYR6I\nXHVXXGQYaRMHfzu+UUNTC1sKysnYd4RPdxTzl/dzyMor428Xz6KfDvPQK/TMivK6Rz/dTYQjhKvn\njyIkRPjF6RN56LI5bC2o4Lx/fkF2Xrnb67j//RzW7S3lzxdMZ/JQvQDLV4Q7Qpg9IpHrFo7h2Z/M\n5c5zpvDh1kNc+MiXHCjVi756gyZ95VX5ZbW88U0+lxw7ggExEd9OP3vGUF694XhCRLjo0S95K7Pn\nN2Z7N7uQZWv2cMW8kZw/O8UTYateICL85MTRPH31XArKajnvoS9Yu+ewt8MKOJr0lVc9vsYaaXvJ\nwqPHsJ86LJ43fzqfGSnx3LziG/7y/nZaWrrX/3t3cRW3vprFrOEJ3HHOZI/ErHrXwgmDePOnJ5IY\nHcbi5et4du1+b4cUUDTpK68pqapnxfpczp+dzLCE9vvKD4yJ4Plr53Hp3OE89MluljybQWVdo0v1\nV9c3sfTZDYQ7Qnj48jkePSmsetfogf14/ab5LJwwiDvf2MzvXs+msbnF22EFBE36ymue/HwvDc0t\nLE3rvOtkuCOEe8+fzt3nTeWTnGIuePhL9pVUd7qMMYbbX8tmd3EV/3fp7A6/VJTviosM4/ErU1l6\n0lieX5fL4uXrOFxV7+2w/J4mfeUVFXWNPPvVfr43bShjB8V0WV5EuPL4UTx7zVyKq+o576Ev+GJX\nSYfln/5yH6syC/jl6ROZrxdg+a3QEOvOXQ/+aBabDpRx7j+/YFthh7fYVi7QpK+84tmv9lNZ38QN\nXRzlt3XCuIG8ddOJJMVFcOWT63nqi71HjfOSsa+Ue97exqLJegFWoPjB7GRevv54mlpauPCRL3lv\nc6G3Q/JbmvRVn6ttaObJz/dy0oRBTEuO7/byIwZE89qN8zl54mD+sGort6/Mpr7Jum9rcWU9N72w\nkWT7Aiy9yCdwzByewKqfnsiEpFiWPreRBz/a0e0T+0qTvvKClzMOcLi6gRu7eZTvLCbCwbIrjuFn\np4zjpYwDXP74Og5V1PGzFRspr23k0cXHEB+lF2AFmsFxkby4ZB4XzknhwY92ctMLG6mub/J2WH5F\nL3lTfaqxuYVla/aQOjKRuaP7u1VXSIjwy9MnMnFILL96JZOF939CfVMLf7t4pl6AFcAiw0L56w9n\nMHloLPe+s429JdU8fmUqw/tHezs0v6BH+qpPvbmpgPyyWm46eZzHxr45Z8YwXl16AkPjI7n2xNFc\nMEcvwAp0IsK1C8bw1NVzybcv5FqnF3K5RJO+6jMtLYZH0ncxeWgcaRMHebTuacnxfPKrNO44R++A\nFUxOmjCIN2+aT0J0GJcvX8erG/K8HZLP06Sv+swHWw+yu7iaG9PG9soIlzpqZnAaMyiGN26az7wx\nA/j1q5mk5xR5OySfpklf9QljDA99sptRA6L53vSh3g5HBZi4yDAeu+IYJg+N46cvfMP2g9qXvyOa\n9FWf+GxnCdn55Sw9aSyh2o1S9YJ+EQ6euOpYYiIcXPPU1xRV1Hk7JJ+kSV/1iYfTdzEkLpLz5yR7\nOxQVwIbER/LEj1Mpq23k2mcyqGnQ7pxtadJXvW7D/iOs3VPKtQtG66BnqtdNHRbP/106m8355dzy\n0ia9gKsNTfqq1z2SvovE6DAunTvC26GoIHHq5CTuPGcK7285xH3vbfd2OD5FL85SvWpbYQUfbSvi\nlkUT9PZ3qk9dPX80+0qqWbZmD6MG9OOy4/SgAzTpq172SPpu+oWHctUJI70digpCd54zhdzSGu58\nczMpiVEsnODZ60P8kTbvqF6z/3A1q7MKWDxvJAnR4d4ORwUhR2gI/3fZHCYkxXLT8xvJOVjp7ZC8\nTpO+6jWPfroHR2gIPzlxtLdDUUEsJsLBE1elEhUeyjVPf01xZXDfiEWTvuoVhyrqWLkhjx8ek8Lg\nuEhvh6OC3LCEKJ646lhKqxu47pkM6hqbvR2S12jSV71i+Wd7aGpp4fqFehMT5Rump8Tz90tmkZlX\nxi9eDt6unJr0lceV1TTw/Lpczp05jBEDdLhb5TtOnzqE331vMu9kH+QvH+R4Oxyv0N47yuOe/nIf\nNQ3N3JA2ztuhKHWUn5w4mr0l1TySvpvRA/px8bHDvR1Sn9Kkrzyqur6Jp77Yx6LJSUwcEuvtcJQ6\niojwh3OncuBILb99PZvkxCjmjxvo7bD6jDbvKI9asT6X8tpGbjxZ2/KV73KEhvDQZbMZOyiGpc9t\nYFdR8HTldCnpi8gtIrJFRDaLyAoRiRSRU0Vko4hsEpHPRaTd3/Ii8hsR2SUiOSJyhmfDV76kvqmZ\nxz/bwwljBzBnRKK3w1GqU7GRYTzx41QiHKFc/fTXlFQFR1fOLpO+iCQDNwOpxphpQChwCfAIcLkx\nZhbwAnBHO8tOsctOBc4EHhYRHXErQL22MZ9DFfXcqG35yk+kJEbzxFWpFFfWsyRIunK62rzjAKJE\nxAFEAwWAAVrvPh1vT2vrPOBFY0y9MWYvsAuY617IyhcZY1i2Zg8zU+KZP26At8NRymUzhyfw4I9m\nsTG3jNtWZmFMYHfl7PJErjEmX0T+CuQCtcAHxpgPRORa4B0RqQUqgHntLJ4MrHV6nWdP+w4RWQIs\nAUhKSiIKz9EAAAAUxklEQVQ9Pb272/Gtqqoqt5ZXPXO4toW9JbUsnhzOp59+6u1wvEb3P/8UCVw4\nPoyVmwqIqS/htJFh3g6p13SZ9EUkEeuIfTRQBrwiIouBC4DvGWPWicitwN+Aa3sShDFmGbAMIDU1\n1aSlpfWkGgDS09NxZ3nVM+9tPghs4MKTU5kdxO35uv/5r4ULDeXPbuClnCIuSDuGY0b293ZIvcKV\n5p1FwF5jTLExphF4DZgPzDTGrLPLvASc0M6y+YBzJ9gUe5oKMNn5ZThChMlD47ourJQPCgkRHrh4\nJsmJUdz4/MaAHaPHlaSfC8wTkWgREeBUYCsQLyIT7DKnAdvaWfYt4BIRiRCR0cB4YL0H4lY+Jiuv\nnAlJsUSG6Xl65b/io8J4dPExlNc28rMVG2lqbvF2SB7XZdK3j+ZfBTYC2fYyy4DrgJUikglcAdwK\nICLnisjd9rJbgJexviTeA24yxgT+6fEgY4whO7+cGSnx3g5FKbdNHhrHvedPZ+2e0oAcqsGlK3KN\nMXcBd7WZ/Lr9aFv2Lawj/NbX9wD3uBGj8nF5R2opq2lkuiZ9FSAumJPCxtwjPPbpHmYPT+TMaUO8\nHZLH6BW5ym1ZeeUAzEhO8HIkSnnOnedMYebwBH71SiZ7iqu8HY7HaNJXbsvKKyM8NIQJQ2K8HYpS\nHhPhCOWRy+cQ7ghh6XMbqGlo8nZIHqFJX7ktK6+cSUNjiXDoSVwVWIYlRPGPS2azq6iK21dmB8SF\nW5r0lVtaWgyb9SSuCmAnjh/IL0+fyFuZBfzry33eDsdtmvSVW/Ydrqayvknb81VAu+GksSyaPJg/\nvb2NDftLvR2OWzTpK7dk51sncbXnjgpk1oVbsxiW4P8XbmnSV27JyisnwhHC+MF6ElcFtvioMB5Z\nPIeyGv++cEuTvnJLdl45U4fF4QjVXUkFvqnD4rnHzy/c0v9U1WPNLYbNBeXMSNH2fBU8LjomhcuO\nG8Fjn+6xBxr0L5r0VY/tKa6ipqGZ6cnanq+Cy13fn8LMlHi/vHBLk77qsW+vxNWTuCrIRDhCeXjx\nMYSFCjc8t9GvLtzSpK96LCuvjOjwUMYM0pO4KvgkJ0Tx90tms6Ookt+85j8XbmnSVz2WlV/OtOR4\nQkPE26Eo5RULJwziF4sm8OamAp75ar+3w3GJJn3VI43NLWwtqGCGtuerIHfTyeM4ddJg/vT2Vjbs\nP+LtcLqkSV/1yM5DVdQ3tehFWSrohYQIf7t4FkPjo7jhuQ0cqqjzdkid0qSveiQ7vwxAu2sqBcRH\nh/HYFcdQVd/E9c9uoK7Rd+8VpUlf9UhWXjmxkQ5G9o/2dihK+YTJQ+N44Icz2XSgjDve2OyzJ3Y1\n6aseyc4vZ3pyPCF6Elepb501fSg3nzqeVzfk8dQX+7wdTrs06atuq29qZlthhbbnK9WO/zp1PKdP\nSeKed7bx+c4Sb4dzFE36qtt2HKyisdnocMpKtSMkRPjbj2YxdlA/bnphI/sPV3s7pO/QpK+6LTOv\n9SSuHukr1Z6YCAePX5mKCFz3TAZV9b5zxa4mfdVt2XnlJEaHkZIY5e1QlPJZIwf046HL5rC7uJpb\nXtpES4tvnNjVpK+6LSu/nOkpCYjoSVylOjN/3EB+973JfLj1EA/+e6e3wwE06atuqmtsZsehSr0S\nVykXXT1/FD88JoV//Hsn72YXejscTfqqe7YWVtDcYrTnjlIuEhH+dP40Zo9I4BcvZ7KtsMKr8WjS\nV92SrcMpK9VtEY5QHlt8DHFRDq57JoPS6gavxaJJX3VLVl45A2MiGBIX6e1QlPIrg+MieeyKVIoq\n67nx+Q00eukeu5r0Vbdk55cxIyVeT+Iq1QOzhidw3wXWPXb/uHqrV2LQpK9cVl3fxK6iKr09olJu\nuGBOCtctGM0zX+1nxfrcPl+/Jn3lsq2FFbQYbc9Xyl23nTmJBeMH8vs3N5Oxr7RP161JX7ks84B1\nJa723FHKPY7QEP556RySE6JY+twGCspq+2zdmvSVy7LzyxkaH8ngWD2Jq5S74qPDePzKVOoaW1jy\nbAa1DX0zBr8mfeWy7Lxybc9XyoPGJ8Xy4I9msaWggttWZvXJGPya9JVLKuoa2VNSre35SnnYoilJ\n/Or0ibyVWcBja/b0+vo06SuXbM63LsqarrdHVMrjbkwby9kzhvLl7sO9PjCbo1drVwGj9Upcbd5R\nyvNEhAd+OBNHiPT63eg06SuXZOWXk5IYRf9+4d4ORamAFBkW2ifrcSnpi8gtwLWAAbKBq4EPgVi7\nyGBgvTHmB+0s22wvA5BrjDnX3aBV38vOK9f2fKUCQJdJX0SSgZuBKcaYWhF5GbjEGLPAqcxK4M0O\nqqg1xszySLTKK8pqGsgtreHSuSO8HYpSyk2unsh1AFEi4gCigYLWGSISB5wCvOH58JQvyM7XkTWV\nChRdJn1jTD7wVyAXKATKjTEfOBX5AfBvY0xHg0RHikiGiKwVkaOaf5Tvy7JP4k7Tk7hK+T1XmncS\ngfOA0UAZ8IqILDbGPGcXuRRY3kkVI40x+SIyBvhYRLKNMbvbrGMJsAQgKSmJ9PT07m+Jraqqyq3l\n1dE+3lRHUrTwzbovvB2Kz9P9T/k6V07kLgL2GmOKAUTkNeAE4DkRGQjMBc7vaGH7lwLGmD0ikg7M\nBna3KbMMWAaQmppq0tLSur0hrdLT03FneXW03371b44b35+0tNneDsXn6f6nfJ0rbfq5wDwRiRZr\nEPVTgW32vIuA1caYuvYWFJFEEYmwnw8E5gPeGURa9UhxZT0F5XXanq9UgHClTX8d8CqwEavrZQj2\nUTlwCbDCubyIpIpIa3PPZCBDRDKBT4D7jDGa9P3It1fianu+UgHBpX76xpi7gLvamZ7WzrQMrD79\nGGO+BKa7F6Lypqy8ckRgqiZ9pQKCjr0TQN7bXMjB8nZb2nosO7+MsYNiiInQi7eVCgSa9APEgdIa\nlj63kZte2OjRAZuy8sqZoUf5SgUMTfoBYlWWdb3chv1HeMFD9908VFFHUWW93ilLqQCiST9ArMos\nZNbwBOaPG8D/vLudQxXuN/O03h5xhg6nrFTA0KQfAHYVVbGtsIJzZw7jnh9Mp6G5hf9+a4vb9Wbn\nlxMaIkwZGueBKJVSvkCTfgBYnVWACJw9YyijBvbj54vG8+7mg3yw5aBb9WbllTN+cAxR4X0z5KtS\nqvdp0vdzxhhWZRYwd1R/kuKsG5Zft2AMk4bE8vs3t1BZ19jjerPzdThlpQKNJn0/t/1gJbuLq/n+\nzGHfTgsLDeG+C2dwqLKOv76f06N688tqKa1u0NsjKhVgNOn7uVWZBYSGCGdNG/Kd6bOGJ3DV8aN4\nZu1+NuYe6Xa9rbdH1O6aSgUWTfp+zBjD6qxCThg7gAExEUfN/9UZExkSF8lvVmbT2NzSrbqz8ssJ\nCxUmDY3turBSym9o0vdjWXnl5JbW8P0Zw9qdHxPh4I/nTSPnUCXL1uzpVt3ZeeVMHBJLhENP4ioV\nSDTp+7HVWQWEhQpnTB3SYZlFU5L43vQh/P3fO9lbUu1SvcYYsvLKmJ6s7flKBRpN+n6qpcVq2lk4\nfhDx0WGdlv3v708lwhHC717Pxpiuh2jILa2hoq6JmdpzR6mAo0nfT23MPUJhed13eu10ZHBcJLef\nNYkvdx/m1Q15XZbPtE/i6vALSgUeTfp+alVmARGOEBZNSXKp/KXHjiB1ZCL3vLONkqr6Tstm55UR\n7ghhQpKexFUq0GjS90PNLYa3sw9yyqTBLg95HBIi/PmC6VTXN/Gn1Z3fxyYrr5wpQ+MIC9XdQ6lA\no//VfmjdnsOUVNW71LTjbHxSLDekjeONTQV8uqO43TItLYbNeiWuUgFLk74fWpVVQHR4KCdPHNzt\nZW86eSxjBvXjd69nU9PQdNT8PSXVVDc06+0RlQpQmvT9TGNzC+9uPshpU5J6NBBahCOUP58/nbwj\ntfz9o51Hzc/O1+GUlQpkmvT9zOe7SiiraeScDi7IcsVxYwZw6dzhLP9877c3Pm+VlVdOVFgoYwf1\nczdUpZQP0qTvZ1ZnFhIb6WDhhIFu1XP7mZNJjA7nN69l0+Q0REN2XjlTh8Xh0JO4SgUk/c/2I3WN\nzXyw5SBnTB3i9vAI8dFh/Pe5U8jOL+fpL/cB0NTcwpaCCm3aUSqAadL3I2t2FFNZ39TtXjsdOXv6\nUE6ZNJgHPtjBgdIadhdXU9vYrD13lApgmvT9yKqsQvr3C+eEsQM8Up+IcPd5UxGBO9/cTGaedRJX\nr8RVKnC5dmWP8rqahiY+2nqI8+cke/SiqZTEaH55+kT+uHor+0qqiYlwMHqAnsRVKlDpkb6f+Hh7\nEbWNzR0Oo+yOH58wihkp8ew7XMO05DhCQsTj61BK+QZN+n5idWYhg2IjmDu6v8frDrWHaAgNEWaP\nSPR4/Uop36HNO36gsq6Rj3OKuGzuCEJ76Sh86rB43rl5AcmJUb1Sv1LKN2jS9wMfbj1EQ1ML3585\ntFfXM3GIjqqpVKDT5h0/sDqrkOSEKGYP16YXpZR7NOn7uLKaBtbsKOacGUP1BKtSym2a9H3c+1sO\n0tRi3BprRymlWmnS93GrMgsZNSCaaclx3g5FKRUANOn7sJKqer7cXcI5M4Yhok07Sin3adL3Ye9m\nF9Ji8NhYO0oppUnfh63KKmT84BjtSqmU8hhN+j7qYHkdX+8r1aN8pZRHadL3UW9nF2IMnDOjdy/I\nUkoFF5eSvojcIiJbRGSziKwQkUgR+UxENtmPAhF5o4NlrxKRnfbjKs+GH7hWZRYwdVgcYwbFeDsU\npVQA6XIYBhFJBm4GphhjakXkZeASY8wCpzIrgTfbWbY/cBeQChhgg4i8ZYw54qkNCEQHSmvYdKCM\n28+a5O1QlFIBxtXmHQcQJSIOIBooaJ0hInHAKUB7R/pnAB8aY0rtRP8hcKZ7IfsmYwx5R2poaGrp\nunAXVmcVAtadrZRSypO6PNI3xuSLyF+BXKAW+MAY84FTkR8A/zbGVLSzeDJwwOl1nj0tINQ1NvPV\n7sN8vL2IT3KKyDtSy8CYcC5OHc6lc0cwvH90j+pdlVnA7BEJPV5eKaU64krzTiJwHjAaKANeEZHF\nxpjn7CKXAsvdCUJElgBLAJKSkkhPT+9xXVVVVW4t35XDtS1kFjeTWdzMtsPNNLRAeChMHRDKgknh\nbDvczCPpu3kkfTczBoVy8nAHMwaFEuLixVWFVS1sLazl0knhvbodqnf09v6nlLtcGVp5EbDXGFMM\nICKvAScAz4nIQGAucH4Hy+YDaU6vU4D0toWMMcuAZQCpqakmLS2tbRGXpaen487ybTU1t7Bh/xE+\nzikifXsxOYeqARg5IJrL5g3m5EmDOW50fyLDQr9dJr+slhfX5/Li1wd4cGM9yQlRXHbccC5OHc6g\n2IhO1/f3j3YisoOfn7+AIfGRHtsO1Tc8vf8p5WmuJP1cYJ6IRGM175wKZNjzLgJWG2PqOlj2feBe\n+9cCwOnAb9yIt0P1Tc089cU+CnMbqcgsIDbSQVykg9jIMGLtv/3CQ10azuBwVT2f7ijm4+1FrNlR\nTEVdE44QYe7o/tyROpmTJw1mzMB+HdaVnBDFL0+fyM2njueDLYd4bu1+/vJ+Dg9+tIMzpg5h8byR\nHDe6/1HLG2NYlVXAsaP6a8JXSvUKV9r014nIq8BGoAn4BvuoHLgEuM+5vIikAkuNMdcaY0pF5I/A\n1/bsu40xpR6L3kl5TSP3vbsdgH9t/abdMiECMRH/+SKI+/YLwZoWGRZCxv4jbDpQhjEwKDaCM6cN\n4eSJgzlx/EBiI8O6FVNYaAhnzxjK2TOGsquoihfW5fLqhgOstq+0vfy4EZw/J4X4KKvenEOV7Cqq\n4o8/mObem6GUUh0QY4y3Y/iO1NRUk5GR0XXBNowx1DQ088Ena5g2+1gq6pqorGuksq7JfjR+5+93\n5tdbf2vqm5k8LI5TJg7mlEmDmTrM8zcJr21oZlVWAc+vyyXzQBlRYaGcO3MYi+eN5P0tB3k4fRfr\nf7eIgTGdNwMp36TNO8pbRGSDMSa1q3IBc7tEEaFfhIPEyBDGJ/nuWDVR4aFcnGq172fnlfP8uv28\nuamAlzIO4AgR5o8bqAlfKdVrAibp+6PpKfHclzKD3549mdc35vNWZgE/OXG0t8NSSgUwTfo+IC4y\njKtOGMVVJ4zydihKqQCnA64ppVQQ0aSvlFJBRJO+UkoFEU36SikVRDTpK6VUENGkr5RSQUSTvlJK\nBRFN+kopFUR8buwdESkHdnZSJB4o72T+QKDEo0H1ra62z9fX52593V2+O+VdKetuGd3/vLu+vt7/\nurOMp8p1NH+kMWZQl7UbY3zqASxzc36Gt7ehN7ff19fnbn3dXb475V0p624Z3f+8u76+3v+6s4yn\nyrm7jb7YvLPKzfn+rq+3z9Prc7e+7i7fnfKulPVUGX+l+1/vLeOpcm5to88177hLRDKMC8OLKtUb\ndP9Tvs4Xj/TdtazrIkr1Gt3/lE8LuCN9pZRSHQvEI32llFId0KSvlFJBRJO+UkoFkaBK+iLST0Qy\nROQcb8eigo+ITBaRR0XkVRG5wdvxqODkF0lfRJ4UkSIR2dxm+pkikiMiu0Tkdhequg14uXeiVIHM\nE/ugMWabMWYpcDEwvzfjVaojftF7R0QWAlXAM8aYafa0UGAHcBqQB3wNXAqEAn9uU8U1wExgABAJ\nlBhjVvdN9CoQeGIfNMYUici5wA3As8aYF/oqfqVa+cWN0Y0xa0RkVJvJc4Fdxpg9ACLyInCeMebP\nwFHNNyKSBvQDpgC1IvKOMaalN+NWgcMT+6Bdz1vAWyLyNqBJX/U5v0j6HUgGDji9zgOO66iwMeZ3\nACLyY6wjfU34yl3d2gftA48LgAjgnV6NTKkO+HPS7xFjzNPejkEFJ2NMOpDu5TBUkPOLE7kdyAeG\nO71Osacp1Vd0H1R+x5+T/tfAeBEZLSLhwCXAW16OSQUX3QeV3/GLpC8iK4CvgIkikiciPzHGNAE/\nBd4HtgEvG2O2eDNOFbh0H1SBwi+6bCqllPIMvzjSV0op5Rma9JVSKoho0ldKqSCiSV8ppYKIJn2l\nlAoimvSVUiqIaNJXSqkgoklfKaWCiCZ9pZQKIv8PpJIcsBMmFIMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ffa6007c290>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.semilogx(reg_vals, acc_val)\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization (logistic)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Conclusion__: A regularization value of 1e-3 provides the best accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network\n",
    "batch_size = 128\n",
    "n_hidden = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Input data.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    reg = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # Variables\n",
    "    weights1 = tf.Variable(\n",
    "        tf.truncated_normal([image_size * image_size, n_hidden]))\n",
    "    biases1 = tf.Variable(tf.zeros([n_hidden]))\n",
    "    weights2 = tf.Variable(\n",
    "        tf.truncated_normal([n_hidden, num_labels]))\n",
    "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    # Training computation\n",
    "    hidden = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "    logits = tf.matmul(hidden, weights2) + biases2\n",
    "    loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) + reg * tf.nn.l2_loss(weights2)\n",
    "    \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    # Predictions for the training, validation and test data\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(\n",
    "        tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1), weights2) + biases2)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1), \n",
    "                                              weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 307.783783\n",
      "Minibatch accuracy: 5.5%\n",
      "Validation accuracy: 25.9%\n",
      "Minibatch loss at step 500: 17.740042\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 78.8%\n",
      "Minibatch loss at step 1000: 7.182632\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 81.9%\n",
      "Minibatch loss at step 1500: 15.245363\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 79.5%\n",
      "Minibatch loss at step 2000: 6.147029\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 79.6%\n",
      "Minibatch loss at step 2500: 2.588814\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 81.4%\n",
      "Minibatch loss at step 3000: 7.935167\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 81.5%\n",
      "Test accuracy: 89.2%\n"
     ]
    }
   ],
   "source": [
    "num_steps= 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, reg: 1e-4}\n",
    "        _, l, predictions = session.run(\n",
    "            [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "                valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Let us plot the accuracy for various values of regularization\"\"\"\n",
    "import numpy as np\n",
    "import sys\n",
    "num_steps = 3001\n",
    "reg_vals = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "acc_val = []\n",
    "\n",
    "for reg_i in reg_vals:\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        for step in range(num_steps):\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, reg: reg_i}\n",
    "            _, l, predictions = session.run(\n",
    "                [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        acc_val.append(accuracy(test_prediction.eval(), test_labels))\n",
    "print(len(acc_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEMCAYAAAAoB2Y1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ffa24520a50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.semilogx(reg_vals, acc_val)\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization (logistic)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Neural Network\n",
    "batch_size = 128\n",
    "n_hidden = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Input data.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    # Variables\n",
    "    weights1 = tf.Variable(\n",
    "        tf.truncated_normal([image_size * image_size, n_hidden]))\n",
    "    biases1 = tf.Variable(tf.zeros([n_hidden]))\n",
    "    weights2 = tf.Variable(\n",
    "        tf.truncated_normal([n_hidden, num_labels]))\n",
    "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    # Training computation\n",
    "    hidden = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "    logits = tf.matmul(hidden, weights2) + biases2\n",
    "    loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    # Predictions for the training, validation and test data\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(\n",
    "        tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1), weights2) + biases2)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1), \n",
    "                                              weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 370.531952\n",
      "Minibatch accuracy: 3.9%\n",
      "Validation accuracy: 22.8%\n",
      "Minibatch loss at step 2: 1350.793091\n",
      "Minibatch accuracy: 33.6%\n",
      "Validation accuracy: 38.3%\n",
      "Minibatch loss at step 4: 339.455444\n",
      "Minibatch accuracy: 57.8%\n",
      "Validation accuracy: 64.8%\n",
      "Minibatch loss at step 6: 90.703896\n",
      "Minibatch accuracy: 70.3%\n",
      "Validation accuracy: 71.5%\n",
      "Minibatch loss at step 8: 23.779957\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 74.9%\n",
      "Minibatch loss at step 10: 56.901497\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 74.2%\n",
      "Minibatch loss at step 12: 37.737587\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 74.9%\n",
      "Minibatch loss at step 14: 9.367664\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 75.7%\n",
      "Minibatch loss at step 16: 7.330304\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 75.5%\n",
      "Minibatch loss at step 18: 19.580494\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 76.1%\n",
      "Minibatch loss at step 20: 0.000001\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.2%\n",
      "Minibatch loss at step 22: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.3%\n",
      "Minibatch loss at step 24: 3.352738\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 75.9%\n",
      "Minibatch loss at step 26: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.4%\n",
      "Minibatch loss at step 28: 2.736543\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 76.3%\n",
      "Minibatch loss at step 30: 2.350798\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 75.8%\n",
      "Minibatch loss at step 32: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.3%\n",
      "Minibatch loss at step 34: 2.489318\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 76.3%\n",
      "Minibatch loss at step 36: 1.659544\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 75.7%\n",
      "Minibatch loss at step 38: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.3%\n",
      "Minibatch loss at step 40: 2.245714\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 76.2%\n",
      "Minibatch loss at step 42: 1.072975\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 74.3%\n",
      "Minibatch loss at step 44: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.2%\n",
      "Minibatch loss at step 46: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.3%\n",
      "Minibatch loss at step 48: 0.613647\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 75.8%\n",
      "Minibatch loss at step 50: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.3%\n",
      "Minibatch loss at step 52: 2.143131\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 76.3%\n",
      "Minibatch loss at step 54: 0.149580\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 75.5%\n",
      "Minibatch loss at step 56: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.2%\n",
      "Minibatch loss at step 58: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.2%\n",
      "Minibatch loss at step 60: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.2%\n",
      "Minibatch loss at step 62: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.2%\n",
      "Minibatch loss at step 64: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.2%\n",
      "Minibatch loss at step 66: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.2%\n",
      "Minibatch loss at step 68: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.2%\n",
      "Minibatch loss at step 70: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.2%\n",
      "Minibatch loss at step 72: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.2%\n",
      "Minibatch loss at step 74: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.2%\n",
      "Minibatch loss at step 76: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.2%\n",
      "Minibatch loss at step 78: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.2%\n",
      "Minibatch loss at step 80: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.2%\n",
      "Minibatch loss at step 82: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.2%\n",
      "Minibatch loss at step 84: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.2%\n",
      "Minibatch loss at step 86: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.2%\n",
      "Minibatch loss at step 88: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.2%\n",
      "Minibatch loss at step 90: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.2%\n",
      "Minibatch loss at step 92: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.2%\n",
      "Minibatch loss at step 94: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.2%\n",
      "Minibatch loss at step 96: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.2%\n",
      "Minibatch loss at step 98: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.2%\n",
      "Minibatch loss at step 100: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.2%\n",
      "Test accuracy: 83.1%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 101\n",
    "num_batches = 3\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        #offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        offset = ((step % num_batches) * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "            [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 2 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "                valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Neural Network\n",
    "batch_size = 128\n",
    "n_hidden = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Input data.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    # Variables\n",
    "    weights1 = tf.Variable(\n",
    "        tf.truncated_normal([image_size * image_size, n_hidden]))\n",
    "    biases1 = tf.Variable(tf.zeros([n_hidden]))\n",
    "    weights2 = tf.Variable(\n",
    "        tf.truncated_normal([n_hidden, num_labels]))\n",
    "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    # Training computation\n",
    "    hidden = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "    drop1 = tf.nn.dropout(hidden, 0.5)\n",
    "    logits = tf.matmul(drop1, weights2) + biases2\n",
    "    loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    # Predictions for the training, validation and test data\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(\n",
    "        tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1), weights2) + biases2)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1), \n",
    "                                              weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 421.579651\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 24.7%\n",
      "Minibatch loss at step 2: 1405.446045\n",
      "Minibatch accuracy: 34.4%\n",
      "Validation accuracy: 37.9%\n",
      "Minibatch loss at step 4: 319.498474\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy: 57.0%\n",
      "Minibatch loss at step 6: 55.871067\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 58.1%\n",
      "Minibatch loss at step 8: 27.072853\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 61.0%\n",
      "Minibatch loss at step 10: 43.429752\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 63.0%\n",
      "Minibatch loss at step 12: 0.394500\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 65.5%\n",
      "Minibatch loss at step 14: 7.298527\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 63.9%\n",
      "Minibatch loss at step 16: 2.784857\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 65.4%\n",
      "Minibatch loss at step 18: 6.877446\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 66.6%\n",
      "Minibatch loss at step 20: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.9%\n",
      "Minibatch loss at step 22: 1.512695\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 67.2%\n",
      "Minibatch loss at step 24: 1.403793\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 66.7%\n",
      "Minibatch loss at step 26: 0.185192\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 67.0%\n",
      "Minibatch loss at step 28: 13.669550\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 64.7%\n",
      "Minibatch loss at step 30: 12.458252\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 66.7%\n",
      "Minibatch loss at step 32: 1.330917\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 67.0%\n",
      "Minibatch loss at step 34: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.8%\n",
      "Minibatch loss at step 36: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.2%\n",
      "Minibatch loss at step 38: 3.444601\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 67.0%\n",
      "Minibatch loss at step 40: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.9%\n",
      "Minibatch loss at step 42: 0.178925\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 67.3%\n",
      "Minibatch loss at step 44: 0.000021\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.3%\n",
      "Minibatch loss at step 46: 1.014332\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 66.1%\n",
      "Minibatch loss at step 48: 0.000777\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.1%\n",
      "Minibatch loss at step 50: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.1%\n",
      "Minibatch loss at step 52: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.3%\n",
      "Minibatch loss at step 54: 0.608754\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 66.7%\n",
      "Minibatch loss at step 56: 0.414799\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 66.5%\n",
      "Minibatch loss at step 58: 23.348061\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 67.0%\n",
      "Minibatch loss at step 60: 2.675825\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 66.6%\n",
      "Minibatch loss at step 62: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.6%\n",
      "Minibatch loss at step 64: 0.050949\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 66.5%\n",
      "Minibatch loss at step 66: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.5%\n",
      "Minibatch loss at step 68: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.3%\n",
      "Minibatch loss at step 70: 1.257977\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 67.2%\n",
      "Minibatch loss at step 72: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.3%\n",
      "Minibatch loss at step 74: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.3%\n",
      "Minibatch loss at step 76: 0.127320\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 66.6%\n",
      "Minibatch loss at step 78: 1.051077\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 66.9%\n",
      "Minibatch loss at step 80: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.9%\n",
      "Minibatch loss at step 82: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.9%\n",
      "Minibatch loss at step 84: 0.397319\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 66.7%\n",
      "Minibatch loss at step 86: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.7%\n",
      "Minibatch loss at step 88: 0.133515\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 66.9%\n",
      "Minibatch loss at step 90: 0.000816\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.9%\n",
      "Minibatch loss at step 92: 0.083768\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 67.5%\n",
      "Minibatch loss at step 94: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.5%\n",
      "Minibatch loss at step 96: 0.138408\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 67.1%\n",
      "Minibatch loss at step 98: 0.366567\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 66.9%\n",
      "Minibatch loss at step 100: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.9%\n",
      "Test accuracy: 74.0%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 101\n",
    "num_batches = 3\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        offset = step % num_batches\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "            [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 2 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "                valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying a 2 layered neural net\n",
    "batch_size = 128\n",
    "num_hidden_nodes1 = 1024\n",
    "num_hidden_nodes2 = 100\n",
    "beta_regul = 1e-3\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    global_step = tf.Variable(0)\n",
    "\n",
    "    # Variables.\n",
    "    weights1 = tf.Variable(\n",
    "    tf.truncated_normal(\n",
    "        [image_size * image_size, num_hidden_nodes1],\n",
    "        stddev=np.sqrt(2.0 / (image_size * image_size)))\n",
    "    )\n",
    "    biases1 = tf.Variable(tf.zeros([num_hidden_nodes1]))\n",
    "    weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes1, num_hidden_nodes2], stddev=np.sqrt(2.0 / num_hidden_nodes1)))\n",
    "    biases2 = tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
    "    weights3 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes2, num_labels], stddev=np.sqrt(2.0 / num_hidden_nodes2)))\n",
    "    biases3 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Training computation.\n",
    "    lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "    lay2_train = tf.nn.relu(tf.matmul(lay1_train, weights2) + biases2)\n",
    "    logits = tf.matmul(lay2_train, weights3) + biases3\n",
    "    loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels)) + \\\n",
    "      beta_regul * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2) + tf.nn.l2_loss(weights3))\n",
    "\n",
    "    # Optimizer.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, 1000, 0.65, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "    lay2_valid = tf.nn.relu(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(lay2_valid, weights3) + biases3)\n",
    "    lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "    lay2_test = tf.nn.relu(tf.matmul(lay1_test, weights2) + biases2)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(lay2_test, weights3) + biases3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3.296391\n",
      "Minibatch accuracy: 7.8%\n",
      "Validation accuracy: 33.7%\n",
      "Minibatch loss at step 500: 1.116970\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 85.0%\n",
      "Minibatch loss at step 1000: 0.900054\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 1500: 0.639733\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 2000: 0.607548\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 2500: 0.513790\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 3000: 0.509243\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 3500: 0.507782\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 4000: 0.513298\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 4500: 0.498828\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 5000: 0.444852\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 5500: 0.442828\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 6000: 0.343429\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 6500: 0.449723\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 7000: 0.475006\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 7500: 0.392155\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 8000: 0.469999\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 8500: 0.533312\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 9000: 0.384304\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.4%\n",
      "Test accuracy: 95.7%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 9001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "                valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Conclusion__: Increasing a layer along with adding dropout helped in achieving a much better accuracy."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
